{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YYYVHSwGFdbj"
   },
   "source": [
    "## Todo list\n",
    "1. Text Preprocess - done\n",
    "2. Classify Tweet Function: Trump vs Staff - done\n",
    "3. Complex features vector -done\n",
    "4. Build Models\n",
    "  * LogisticRegression - done\n",
    "  * SVC - done\n",
    "  * XGBClassifier\n",
    "  * FFNN\n",
    "  * BERT\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "kEVPMcrM0uui",
    "outputId": "3f1ffb32-3104-439f-9ca3-464d0ae6ae70"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing installation: transformers 4.52.4\n",
      "Uninstalling transformers-4.52.4:\n",
      "  Successfully uninstalled transformers-4.52.4\n",
      "\u001b[33mWARNING: Skipping transformerss as it is not installed.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Skipping jax as it is not installed.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Skipping jaxlib as it is not installed.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Skipping flax as it is not installed.\u001b[0m\u001b[33m\n",
      "\u001b[0mFound existing installation: tokenizers 0.21.1\n",
      "Uninstalling tokenizers-0.21.1:\n",
      "  Successfully uninstalled tokenizers-0.21.1\n",
      "Found existing installation: huggingface-hub 0.32.4\n",
      "Uninstalling huggingface-hub-0.32.4:\n",
      "  Successfully uninstalled huggingface-hub-0.32.4\n",
      "Collecting transformers==4.52.4 (from transformers[sentencepiece,torch]==4.52.4)\n",
      "  Downloading transformers-4.52.4-py3-none-any.whl.metadata (38 kB)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers==4.52.4->transformers[sentencepiece,torch]==4.52.4) (3.18.0)\n",
      "Collecting huggingface-hub<1.0,>=0.30.0 (from transformers==4.52.4->transformers[sentencepiece,torch]==4.52.4)\n",
      "  Downloading huggingface_hub-0.32.4-py3-none-any.whl.metadata (14 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers==4.52.4->transformers[sentencepiece,torch]==4.52.4) (1.24.3)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers==4.52.4->transformers[sentencepiece,torch]==4.52.4) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers==4.52.4->transformers[sentencepiece,torch]==4.52.4) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers==4.52.4->transformers[sentencepiece,torch]==4.52.4) (2024.11.6)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers==4.52.4->transformers[sentencepiece,torch]==4.52.4) (2.32.3)\n",
      "Collecting tokenizers<0.22,>=0.21 (from transformers==4.52.4->transformers[sentencepiece,torch]==4.52.4)\n",
      "  Downloading tokenizers-0.21.1-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.8 kB)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers==4.52.4->transformers[sentencepiece,torch]==4.52.4) (0.5.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers==4.52.4->transformers[sentencepiece,torch]==4.52.4) (4.67.1)\n",
      "Requirement already satisfied: sentencepiece!=0.1.92,>=0.1.91 in /usr/local/lib/python3.11/dist-packages (from transformers[sentencepiece,torch]==4.52.4) (0.2.0)\n",
      "Requirement already satisfied: protobuf in /usr/local/lib/python3.11/dist-packages (from transformers[sentencepiece,torch]==4.52.4) (5.29.5)\n",
      "Collecting torch<2.7,>=2.1 (from transformers[sentencepiece,torch]==4.52.4)\n",
      "  Downloading torch-2.6.0-cp311-cp311-manylinux1_x86_64.whl.metadata (28 kB)\n",
      "Requirement already satisfied: accelerate>=0.26.0 in /usr/local/lib/python3.11/dist-packages (from transformers[sentencepiece,torch]==4.52.4) (1.7.0)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from accelerate>=0.26.0->transformers[sentencepiece,torch]==4.52.4) (5.9.5)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers==4.52.4->transformers[sentencepiece,torch]==4.52.4) (2025.3.2)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers==4.52.4->transformers[sentencepiece,torch]==4.52.4) (4.14.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers==4.52.4->transformers[sentencepiece,torch]==4.52.4) (1.1.2)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch<2.7,>=2.1->transformers[sentencepiece,torch]==4.52.4) (3.5)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch<2.7,>=2.1->transformers[sentencepiece,torch]==4.52.4) (3.1.6)\n",
      "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch<2.7,>=2.1->transformers[sentencepiece,torch]==4.52.4)\n",
      "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch<2.7,>=2.1->transformers[sentencepiece,torch]==4.52.4)\n",
      "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch<2.7,>=2.1->transformers[sentencepiece,torch]==4.52.4)\n",
      "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch<2.7,>=2.1->transformers[sentencepiece,torch]==4.52.4)\n",
      "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch<2.7,>=2.1->transformers[sentencepiece,torch]==4.52.4)\n",
      "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch<2.7,>=2.1->transformers[sentencepiece,torch]==4.52.4)\n",
      "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-curand-cu12==10.3.5.147 (from torch<2.7,>=2.1->transformers[sentencepiece,torch]==4.52.4)\n",
      "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch<2.7,>=2.1->transformers[sentencepiece,torch]==4.52.4)\n",
      "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch<2.7,>=2.1->transformers[sentencepiece,torch]==4.52.4)\n",
      "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cusparselt-cu12==0.6.2 (from torch<2.7,>=2.1->transformers[sentencepiece,torch]==4.52.4)\n",
      "  Downloading nvidia_cusparselt_cu12-0.6.2-py3-none-manylinux2014_x86_64.whl.metadata (6.8 kB)\n",
      "Collecting nvidia-nccl-cu12==2.21.5 (from torch<2.7,>=2.1->transformers[sentencepiece,torch]==4.52.4)\n",
      "  Downloading nvidia_nccl_cu12-2.21.5-py3-none-manylinux2014_x86_64.whl.metadata (1.8 kB)\n",
      "Collecting nvidia-nvtx-cu12==12.4.127 (from torch<2.7,>=2.1->transformers[sentencepiece,torch]==4.52.4)\n",
      "  Downloading nvidia_nvtx_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch<2.7,>=2.1->transformers[sentencepiece,torch]==4.52.4)\n",
      "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting triton==3.2.0 (from torch<2.7,>=2.1->transformers[sentencepiece,torch]==4.52.4)\n",
      "  Downloading triton-3.2.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.4 kB)\n",
      "Collecting sympy==1.13.1 (from torch<2.7,>=2.1->transformers[sentencepiece,torch]==4.52.4)\n",
      "  Downloading sympy-1.13.1-py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch<2.7,>=2.1->transformers[sentencepiece,torch]==4.52.4) (1.3.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.52.4->transformers[sentencepiece,torch]==4.52.4) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.52.4->transformers[sentencepiece,torch]==4.52.4) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.52.4->transformers[sentencepiece,torch]==4.52.4) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.52.4->transformers[sentencepiece,torch]==4.52.4) (2025.4.26)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch<2.7,>=2.1->transformers[sentencepiece,torch]==4.52.4) (3.0.2)\n",
      "Downloading transformers-4.52.4-py3-none-any.whl (10.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.5/10.5 MB\u001b[0m \u001b[31m294.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading huggingface_hub-0.32.4-py3-none-any.whl (512 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m512.1/512.1 kB\u001b[0m \u001b[31m383.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tokenizers-0.21.1-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m367.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading torch-2.6.0-cp311-cp311-manylinux1_x86_64.whl (766.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m766.7/766.7 MB\u001b[0m \u001b[31m247.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m267.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m159.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m269.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m316.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m204.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m260.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m267.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m348.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cusparselt_cu12-0.6.2-py3-none-manylinux2014_x86_64.whl (150.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m150.1/150.1 MB\u001b[0m \u001b[31m302.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_nccl_cu12-2.21.5-py3-none-manylinux2014_x86_64.whl (188.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m188.7/188.7 MB\u001b[0m \u001b[31m173.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m256.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_nvtx_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (99 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m343.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading sympy-1.13.1-py3-none-any.whl (6.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.2/6.2 MB\u001b[0m \u001b[31m275.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading triton-3.2.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (253.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m253.2/253.2 MB\u001b[0m \u001b[31m246.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: triton, nvidia-cusparselt-cu12, sympy, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, huggingface-hub, tokenizers, nvidia-cusolver-cu12, transformers, torch\n",
      "  Attempting uninstall: triton\n",
      "    Found existing installation: triton 3.3.1\n",
      "    Uninstalling triton-3.3.1:\n",
      "      Successfully uninstalled triton-3.3.1\n",
      "  Attempting uninstall: nvidia-cusparselt-cu12\n",
      "    Found existing installation: nvidia-cusparselt-cu12 0.6.3\n",
      "    Uninstalling nvidia-cusparselt-cu12-0.6.3:\n",
      "      Successfully uninstalled nvidia-cusparselt-cu12-0.6.3\n",
      "  Attempting uninstall: sympy\n",
      "    Found existing installation: sympy 1.14.0\n",
      "    Uninstalling sympy-1.14.0:\n",
      "      Successfully uninstalled sympy-1.14.0\n",
      "  Attempting uninstall: nvidia-nvtx-cu12\n",
      "    Found existing installation: nvidia-nvtx-cu12 12.6.77\n",
      "    Uninstalling nvidia-nvtx-cu12-12.6.77:\n",
      "      Successfully uninstalled nvidia-nvtx-cu12-12.6.77\n",
      "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
      "    Found existing installation: nvidia-nvjitlink-cu12 12.6.85\n",
      "    Uninstalling nvidia-nvjitlink-cu12-12.6.85:\n",
      "      Successfully uninstalled nvidia-nvjitlink-cu12-12.6.85\n",
      "  Attempting uninstall: nvidia-nccl-cu12\n",
      "    Found existing installation: nvidia-nccl-cu12 2.26.2\n",
      "    Uninstalling nvidia-nccl-cu12-2.26.2:\n",
      "      Successfully uninstalled nvidia-nccl-cu12-2.26.2\n",
      "  Attempting uninstall: nvidia-curand-cu12\n",
      "    Found existing installation: nvidia-curand-cu12 10.3.7.77\n",
      "    Uninstalling nvidia-curand-cu12-10.3.7.77:\n",
      "      Successfully uninstalled nvidia-curand-cu12-10.3.7.77\n",
      "  Attempting uninstall: nvidia-cufft-cu12\n",
      "    Found existing installation: nvidia-cufft-cu12 11.3.0.4\n",
      "    Uninstalling nvidia-cufft-cu12-11.3.0.4:\n",
      "      Successfully uninstalled nvidia-cufft-cu12-11.3.0.4\n",
      "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
      "    Found existing installation: nvidia-cuda-runtime-cu12 12.6.77\n",
      "    Uninstalling nvidia-cuda-runtime-cu12-12.6.77:\n",
      "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.6.77\n",
      "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
      "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.6.77\n",
      "    Uninstalling nvidia-cuda-nvrtc-cu12-12.6.77:\n",
      "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.6.77\n",
      "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
      "    Found existing installation: nvidia-cuda-cupti-cu12 12.6.80\n",
      "    Uninstalling nvidia-cuda-cupti-cu12-12.6.80:\n",
      "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.6.80\n",
      "  Attempting uninstall: nvidia-cublas-cu12\n",
      "    Found existing installation: nvidia-cublas-cu12 12.6.4.1\n",
      "    Uninstalling nvidia-cublas-cu12-12.6.4.1:\n",
      "      Successfully uninstalled nvidia-cublas-cu12-12.6.4.1\n",
      "  Attempting uninstall: nvidia-cusparse-cu12\n",
      "    Found existing installation: nvidia-cusparse-cu12 12.5.4.2\n",
      "    Uninstalling nvidia-cusparse-cu12-12.5.4.2:\n",
      "      Successfully uninstalled nvidia-cusparse-cu12-12.5.4.2\n",
      "  Attempting uninstall: nvidia-cudnn-cu12\n",
      "    Found existing installation: nvidia-cudnn-cu12 9.5.1.17\n",
      "    Uninstalling nvidia-cudnn-cu12-9.5.1.17:\n",
      "      Successfully uninstalled nvidia-cudnn-cu12-9.5.1.17\n",
      "  Attempting uninstall: nvidia-cusolver-cu12\n",
      "    Found existing installation: nvidia-cusolver-cu12 11.7.1.2\n",
      "    Uninstalling nvidia-cusolver-cu12-11.7.1.2:\n",
      "      Successfully uninstalled nvidia-cusolver-cu12-11.7.1.2\n",
      "  Attempting uninstall: torch\n",
      "    Found existing installation: torch 2.7.1\n",
      "    Uninstalling torch-2.7.1:\n",
      "      Successfully uninstalled torch-2.7.1\n",
      "Successfully installed huggingface-hub-0.32.4 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-cusparselt-cu12-0.6.2 nvidia-nccl-cu12-2.21.5 nvidia-nvjitlink-cu12-12.4.127 nvidia-nvtx-cu12-12.4.127 sympy-1.13.1 tokenizers-0.21.1 torch-2.6.0 transformers-4.52.4 triton-3.2.0\n"
     ]
    },
    {
     "data": {
      "application/vnd.colab-display-data+json": {
       "id": "649d23238be54a08b772ad4da89bbb27",
       "pip_warning": {
        "packages": [
         "huggingface_hub",
         "sympy",
         "torch",
         "torchgen",
         "transformers",
         "triton"
        ]
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.52.4)\n",
      "Requirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (2.14.4)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.32.4)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (1.24.3)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (18.1.0)\n",
      "Requirement already satisfied: dill<0.3.8,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.3.7)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.2)\n",
      "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess in /usr/local/lib/python3.11/dist-packages (from datasets) (0.70.15)\n",
      "Requirement already satisfied: fsspec>=2021.11.1 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]>=2021.11.1->datasets) (2025.3.2)\n",
      "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets) (3.11.15)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.6.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (6.4.4)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (0.3.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.20.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (4.14.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (1.1.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.4.26)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
      "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.18.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.14.0)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.5)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.3.2)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.5.8)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch) (11.2.1.3)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch) (10.3.5.147)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch) (11.6.1.9)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch) (12.3.1.170)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.2)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n",
      "Requirement already satisfied: contractions in /usr/local/lib/python3.11/dist-packages (0.1.73)\n",
      "Requirement already satisfied: textsearch>=0.0.21 in /usr/local/lib/python3.11/dist-packages (from contractions) (0.0.24)\n",
      "Requirement already satisfied: anyascii in /usr/local/lib/python3.11/dist-packages (from textsearch>=0.0.21->contractions) (0.3.2)\n",
      "Requirement already satisfied: pyahocorasick in /usr/local/lib/python3.11/dist-packages (from textsearch>=0.0.21->contractions) (2.1.0)\n",
      "Requirement already satisfied: holidays in /usr/local/lib/python3.11/dist-packages (0.74)\n",
      "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.11/dist-packages (from holidays) (2.9.0.post0)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil->holidays) (1.17.0)\n",
      "Requirement already satisfied: optuna in /usr/local/lib/python3.11/dist-packages (4.3.0)\n",
      "Requirement already satisfied: alembic>=1.5.0 in /usr/local/lib/python3.11/dist-packages (from optuna) (1.16.1)\n",
      "Requirement already satisfied: colorlog in /usr/local/lib/python3.11/dist-packages (from optuna) (6.9.0)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from optuna) (1.24.3)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from optuna) (24.2)\n",
      "Requirement already satisfied: sqlalchemy>=1.4.2 in /usr/local/lib/python3.11/dist-packages (from optuna) (2.0.41)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from optuna) (4.67.1)\n",
      "Requirement already satisfied: PyYAML in /usr/local/lib/python3.11/dist-packages (from optuna) (6.0.2)\n",
      "Requirement already satisfied: Mako in /usr/lib/python3/dist-packages (from alembic>=1.5.0->optuna) (1.1.3)\n",
      "Requirement already satisfied: typing-extensions>=4.12 in /usr/local/lib/python3.11/dist-packages (from alembic>=1.5.0->optuna) (4.14.0)\n",
      "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from sqlalchemy>=1.4.2->optuna) (3.2.2)\n",
      "Requirement already satisfied: gdown in /usr/local/lib/python3.11/dist-packages (5.2.0)\n",
      "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.11/dist-packages (from gdown) (4.13.4)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from gdown) (3.18.0)\n",
      "Requirement already satisfied: requests[socks] in /usr/local/lib/python3.11/dist-packages (from gdown) (2.32.3)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from gdown) (4.67.1)\n",
      "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4->gdown) (2.7)\n",
      "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4->gdown) (4.14.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests[socks]->gdown) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests[socks]->gdown) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests[socks]->gdown) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests[socks]->gdown) (2025.4.26)\n",
      "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.11/dist-packages (from requests[socks]->gdown) (1.7.1)\n",
      "Collecting numpy==1.24.3\n",
      "  Using cached numpy-1.24.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.6 kB)\n",
      "Collecting scipy==1.11.4\n",
      "  Using cached scipy-1.11.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (60 kB)\n",
      "Collecting gensim==4.3.1\n",
      "  Using cached gensim-4.3.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (8.4 kB)\n",
      "Collecting smart-open>=1.8.1 (from gensim==4.3.1)\n",
      "  Using cached smart_open-7.1.0-py3-none-any.whl.metadata (24 kB)\n",
      "Collecting wrapt (from smart-open>=1.8.1->gensim==4.3.1)\n",
      "  Using cached wrapt-1.17.2-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.4 kB)\n",
      "Using cached numpy-1.24.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.3 MB)\n",
      "Using cached scipy-1.11.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (36.4 MB)\n",
      "Using cached gensim-4.3.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (26.6 MB)\n",
      "Using cached smart_open-7.1.0-py3-none-any.whl (61 kB)\n",
      "Using cached wrapt-1.17.2-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (83 kB)\n",
      "Installing collected packages: wrapt, numpy, smart-open, scipy, gensim\n",
      "  Attempting uninstall: wrapt\n",
      "    Found existing installation: wrapt 1.17.2\n",
      "    Uninstalling wrapt-1.17.2:\n",
      "      Successfully uninstalled wrapt-1.17.2\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 1.24.3\n",
      "    Uninstalling numpy-1.24.3:\n",
      "      Successfully uninstalled numpy-1.24.3\n",
      "  Attempting uninstall: smart-open\n",
      "    Found existing installation: smart-open 7.1.0\n",
      "    Uninstalling smart-open-7.1.0:\n",
      "      Successfully uninstalled smart-open-7.1.0\n",
      "  Attempting uninstall: scipy\n",
      "    Found existing installation: scipy 1.11.4\n",
      "    Uninstalling scipy-1.11.4:\n",
      "      Successfully uninstalled scipy-1.11.4\n",
      "  Attempting uninstall: gensim\n",
      "    Found existing installation: gensim 4.3.1\n",
      "    Uninstalling gensim-4.3.1:\n",
      "      Successfully uninstalled gensim-4.3.1\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "optax 0.2.4 requires jax>=0.4.27, which is not installed.\n",
      "optax 0.2.4 requires jaxlib>=0.4.27, which is not installed.\n",
      "orbax-checkpoint 0.11.13 requires jax>=0.5.0, which is not installed.\n",
      "dopamine-rl 4.1.2 requires flax>=0.2.0, which is not installed.\n",
      "dopamine-rl 4.1.2 requires jax>=0.1.72, which is not installed.\n",
      "dopamine-rl 4.1.2 requires jaxlib>=0.1.51, which is not installed.\n",
      "chex 0.1.89 requires jax>=0.4.27, which is not installed.\n",
      "chex 0.1.89 requires jaxlib>=0.4.27, which is not installed.\n",
      "treescope 0.1.9 requires numpy>=1.25.2, but you have numpy 1.24.3 which is incompatible.\n",
      "pymc 5.23.0 requires numpy>=1.25.0, but you have numpy 1.24.3 which is incompatible.\n",
      "tsfresh 0.21.0 requires scipy>=1.14.0; python_version >= \"3.10\", but you have scipy 1.11.4 which is incompatible.\n",
      "blosc2 3.3.4 requires numpy>=1.26, but you have numpy 1.24.3 which is incompatible.\n",
      "xarray-einstats 0.9.0 requires numpy>=1.25, but you have numpy 1.24.3 which is incompatible.\n",
      "albumentations 2.0.8 requires numpy>=1.24.4, but you have numpy 1.24.3 which is incompatible.\n",
      "thinc 8.3.6 requires numpy<3.0.0,>=2.0.0, but you have numpy 1.24.3 which is incompatible.\n",
      "albucore 0.0.24 requires numpy>=1.24.4, but you have numpy 1.24.3 which is incompatible.\n",
      "tensorflow 2.18.0 requires numpy<2.1.0,>=1.26.0, but you have numpy 1.24.3 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed gensim-4.3.1 numpy-1.24.3 scipy-1.11.4 smart-open-7.1.0 wrapt-1.17.2\n"
     ]
    },
    {
     "data": {
      "application/vnd.colab-display-data+json": {
       "id": "5f6ce3f0c04b4d8da3f0b1662ddf1226",
       "pip_warning": {
        "packages": [
         "gensim",
         "numpy",
         "smart_open",
         "wrapt"
        ]
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# PyTprch and Transformers installation and import\n",
    "\n",
    "# uninstalling dependances to make sure we have only the right versions of them\n",
    "!pip uninstall -y transformers transformerss\n",
    "!pip uninstall -y jax jaxlib flax\n",
    "!pip uninstall -y tokenizers\n",
    "!pip uninstall -y huggingface-hub\n",
    "# installing the right version of the transformers library\n",
    "!pip install --no-cache-dir \"transformers[sentencepiece,torch]==4.52.4\"\n",
    "# more installations\n",
    "! pip install transformers datasets\n",
    "! pip3 install torch --no-cache-dir\n",
    "! pip install contractions\n",
    "! pip install holidays\n",
    "! pip install optuna\n",
    "! pip install gdown\n",
    "! pip install --force-reinstall numpy==1.24.3 scipy==1.11.4 gensim==4.3.1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7xLgB5fZo6N2",
    "outputId": "7ff5933b-ef42-4441-cfe0-c49500c5b211"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Standard Library\n",
    "import csv\n",
    "import re\n",
    "import requests\n",
    "import urllib.request\n",
    "from datetime import datetime\n",
    "from abc import ABC, abstractmethod\n",
    "from itertools import combinations\n",
    "\n",
    "# Third-Party Packages\n",
    "import contractions\n",
    "import holidays\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from gensim.models import Word2Vec\n",
    "import optuna\n",
    "import inspect\n",
    "import math\n",
    "from datasets import Dataset\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Machine Learning\n",
    "from sklearn.base import BaseEstimator, TransformerMixin, ClassifierMixin\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.metrics import classification_report, f1_score, recall_score, precision_score, accuracy_score, precision_recall_fscore_support\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.svm import SVC\n",
    "from torch.utils.data import DataLoader, TensorDataset, RandomSampler\n",
    "\n",
    "# NLTK\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "\n",
    "# --- Transformers (Hugging Face) ---\n",
    "from scipy.special import softmax\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import TFAutoModelForSequenceClassification\n",
    "from xgboost import XGBClassifier\n",
    "from transformers import (\n",
    "    BertTokenizerFast,\n",
    "    BertForSequenceClassification,\n",
    "    DataCollatorWithPadding,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qnbMIg11JysW"
   },
   "source": [
    "## Import the file\n",
    "download the csv from google to the colab notebook and open it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "R8NWcQOJJ6nt"
   },
   "outputs": [],
   "source": [
    "file_id = '1czyLFgxVHKLj9NU2asruGrp2SlL16svb'\n",
    "\n",
    "# Create a direct download URL to bypass the Drive preview page\n",
    "url = f'https://drive.google.com/uc?export=download&id={file_id}'\n",
    "\n",
    "output = 'trump_train.tsv'\n",
    "\n",
    "# Download and save the file content locally\n",
    "response = requests.get(url)\n",
    "with open(output, 'wb') as f:\n",
    "    f.write(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "vREzyDLlPfhH"
   },
   "outputs": [],
   "source": [
    "file_id = '1qb8IoGnnVmHV2SXx7hg59HA5rtHmGcs1'\n",
    "\n",
    "# Create a direct download URL to bypass the Drive preview page\n",
    "url = f'https://drive.google.com/uc?export=download&id={file_id}'\n",
    "\n",
    "output = 'trump_tweets_test_a.tsv'\n",
    "\n",
    "# Download and save the file content locally\n",
    "response = requests.get(url)\n",
    "with open(output, 'wb') as f:\n",
    "    f.write(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v3itnBlbJjzh"
   },
   "source": [
    "## Text Preprocess\n",
    "1. Pattern-Based Text Normalization using Regular Expressions - Define regular expressions to identify unwanted patterns (e.g., URLs, emails, numbers) and normalize or remove them for cleaner text.\n",
    "2. Contraction expansion - Expand contractions (e.g., \"don't\" → \"do not\").\n",
    "3. Alphanumeric and Punctuation Filtering - Remove all characters except letters, numbers, and common punctuation to simplify the text.\n",
    "4. Lowercasing - Convert all characters to lowercase for uniformity (user choise).\n",
    "5. Lemmatization - Reduce words to their base dictionary form (e.g., \"ran\" → \"run\") using tools like WordNetLemmatizer or spaCy.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "ijvFyzumJjWf"
   },
   "outputs": [],
   "source": [
    "# Initialize lemmatizer and stopwords\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Regex patterns\n",
    "RE_URL = re.compile(r\"http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+\")\n",
    "RE_HTML = re.compile(r'<[^<]+?>')\n",
    "RE_DATE = re.compile(r'\\b\\d{1,2}[/-]\\d{1,2}[/-]\\d{2,4}\\b|\\b\\d{2,4}[/-]\\d{1,2}[/-]\\d{1,2}\\b')\n",
    "RE_DECIMAL = re.compile(r'\\b\\d+\\.\\d+%?\\b')\n",
    "RE_NUMBER_PERCENT = re.compile(r'\\b\\d+%?\\b')\n",
    "RE_SPECIAL_CHAR = re.compile(r'[^0-9a-zA-Z\\s?!.,:\\'\\\"//]+')\n",
    "\n",
    "def Preprocesstext(text, lowercase=False, removestopwords=True):\n",
    "    \"\"\"\n",
    "    Preprocesses a raw input text string by cleaning and normalizing it for NLP tasks.\n",
    "\n",
    "    The function applies the following steps:\n",
    "    1. Fixes mojibake characters and normalizes smart quotes.\n",
    "    2. Expands English contractions (e.g., \"don't\" → \"do not\").\n",
    "    3. Replaces specific patterns with placeholders\n",
    "    4. Removes unwanted characters except letters, numbers, and basic punctuation.\n",
    "    5. Splits text into tokens and optionally:\n",
    "       - Lowercases non-placeholder tokens.\n",
    "       - Removes stopwords (excluding placeholders).\n",
    "       - Lemmatizes tokens (excluding placeholders).\n",
    "    6. Joins the final tokens back into a normalized string.\n",
    "\n",
    "    Args:\n",
    "        text (str): The raw input text to preprocess.\n",
    "        lowercase (bool): If True, converts all non-placeholder tokens to lowercase.\n",
    "        removestopwords (bool): If True, removes English stopwords (except placeholders).\n",
    "\n",
    "    Returns:\n",
    "        str: A cleaned, normalized, and preprocessed text string.\n",
    "    \"\"\"\n",
    "\n",
    "    # Normalize corrupted or special quotes (mojibake cleanup)\n",
    "    quote_normalization_map = {\n",
    "        \"״\": \"\\\"\", \"“\": \"\\\"\", \"”\": \"\\\"\",\n",
    "        \"׳\": \"'\", \"‘\": \"'\", \"’\": \"'\",\n",
    "        'â\\x80\\x9c': '\"', 'â\\x80\\x9d': '\"', 'â\\x80\\x99': \"'\"\n",
    "    }\n",
    "    for original_char, normalized_char in quote_normalization_map.items():\n",
    "        text = text.replace(original_char, normalized_char)\n",
    "\n",
    "    # Expand contractions (e.g., \"it's\" → \"it is\")\n",
    "    text = contractions.fix(text)\n",
    "\n",
    "    # Replace structured patterns with placeholder tags\n",
    "    text = RE_URL.sub('<URL>', text)\n",
    "    text = RE_HTML.sub('<HTML>', text)\n",
    "    text = RE_DATE.sub('<DATE>', text)\n",
    "    text = RE_DECIMAL.sub('<DECIMAL>', text)\n",
    "    text = RE_NUMBER_PERCENT.sub('<NUMBER>', text)\n",
    "\n",
    "    # Remove unwanted characters except alphanumerics and basic punctuation\n",
    "    text = RE_SPECIAL_CHAR.sub('', text)\n",
    "\n",
    "    # Define protected placeholder tokens\n",
    "    PLACEHOLDERS = ['<URL>', '<HTML>', '<DATE>', '<DECIMAL>', '<NUMBER>']\n",
    "\n",
    "    # Tokenize text\n",
    "    tokens = text.split()\n",
    "\n",
    "    # Lowercase only non-placeholder tokens\n",
    "    if lowercase:\n",
    "        tokens = [t.lower() if t not in PLACEHOLDERS else t for t in tokens]\n",
    "\n",
    "    # Optionally remove stopwords (keep placeholders)\n",
    "    if removestopwords:\n",
    "        tokens = [t for t in tokens if t not in stop_words or t in PLACEHOLDERS]\n",
    "\n",
    "    # Lemmatize non-placeholder tokens\n",
    "    tokens = [lemmatizer.lemmatize(t) if t not in PLACEHOLDERS else t for t in tokens]\n",
    "\n",
    "    # Reconstruct and return cleaned string\n",
    "    text = ' '.join(tokens)\n",
    "    return text\n",
    "\n",
    "\n",
    "def is_valid_timestamp(text, format = '%Y-%m-%d %H:%M:%S'):\n",
    "    \"\"\"\n",
    "    Checks if the input string is in the format 'YYYY-MM-DD HH:MM:SS'.\n",
    "\n",
    "    Args:\n",
    "        text (str): The string to validate.\n",
    "\n",
    "    Returns:\n",
    "        bool: True if valid timestamp format, False otherwise.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        datetime.strptime(text, format)\n",
    "        return True\n",
    "    except ValueError:\n",
    "        return False\n",
    "\n",
    "\n",
    "def average_timestamp(timestamps, format='%Y-%m-%d %H:%M:%S'):\n",
    "    \"\"\"\n",
    "    Calculates the average timestamp from a list of strings, skipping invalid ones.\n",
    "\n",
    "    Args:\n",
    "        timestamps (list of str): List of timestamp strings in a consistent format.\n",
    "        format (str): Expected format of each timestamp. Default: '%Y-%m-%d %H:%M:%S'.\n",
    "\n",
    "    Returns:\n",
    "        str: The average timestamp as a string in the same format.\n",
    "             If no valid timestamps are provided, returns current time.\n",
    "    \"\"\"\n",
    "    # Filter and parse only valid timestamps\n",
    "    valid_timestamps = [datetime.strptime(ts, format) for ts in timestamps if is_valid_timestamp(ts, format)]\n",
    "\n",
    "    # Return current time if no valid timestamps\n",
    "    if not valid_timestamps:\n",
    "        return datetime.now().strftime(format)\n",
    "\n",
    "    # Compute average as float (epoch time)\n",
    "    avg_epoch = sum(map(datetime.timestamp, valid_timestamps)) / len(valid_timestamps)\n",
    "\n",
    "    # Convert back to datetime and return formatted string\n",
    "    return datetime.fromtimestamp(avg_epoch).strftime(format)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JzEFtESJdqI0"
   },
   "source": [
    "##Classify Tweet Function: Trump vs Staff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "i-ZMhSBIdu6w"
   },
   "outputs": [],
   "source": [
    "def classify_tweet(device, username, timestamp):\n",
    "    \"\"\"\n",
    "    Classifies a tweet as being from Donald Trump (1), his staff (0), or unclassifiable (None),\n",
    "    based on historical patterns of device usage and authorship cutoff date.\n",
    "\n",
    "    Heuristics used:\n",
    "    - Before April 1, 2017:\n",
    "        - Tweets from @realDonaldTrump using Android are classified as Trump (1)\n",
    "        - All other tweets before this date are assumed to be staff (0)\n",
    "    - On or after April 1, 2017:\n",
    "        - Device data is no longer reliable for authorship; return None\n",
    "\n",
    "    Parameters:\n",
    "        device (str): The device used to send the tweet (e.g., 'Twitter for Android').\n",
    "        username (str): The username of the account (e.g., 'realDonaldTrump').\n",
    "        timestamp (str): Timestamp of the tweet in format 'YYYY-MM-DD HH:MM:SS'.\n",
    "\n",
    "    Returns:\n",
    "        int: 1 if classified as Trump, 0 if classified as staff.\n",
    "        None: If timestamp is invalid, missing data, or after the cutoff date.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Normalize constants\n",
    "        TRUMP_USERNAME = \"realDonaldTrump\".lower()\n",
    "        TRUMP_CUTOFF_DATE = datetime.strptime('2017-04-01', '%Y-%m-%d')\n",
    "\n",
    "        # Parse timestamp\n",
    "        tweet_date = datetime.strptime(timestamp, '%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "        # Classification logic\n",
    "        if username.lower() == TRUMP_USERNAME and 'android' in device.lower() and tweet_date < TRUMP_CUTOFF_DATE:\n",
    "            return 0  # Likely Trump\n",
    "        elif tweet_date >= TRUMP_CUTOFF_DATE:\n",
    "            return None  # Post-cutoff: not classifiable\n",
    "        else:\n",
    "            return 1  # Likely staff\n",
    "    except Exception:\n",
    "        return None  # Any parsing or data issue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "if2NWMxHih39"
   },
   "source": [
    "## Complex features vector\n",
    "This vector is designed to dynamically integrate various types of features into the model input during the optimization process.\n",
    "It supports combining independent metadata (e.g., device, time), text-based representations like TF-IDF, and semantic embeddings such as Word2Vec, allowing flexible experimentation across different models and configurations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "aAWTj-oGikZu"
   },
   "outputs": [],
   "source": [
    "class AdditionalFeatures(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, country=\"US\"):\n",
    "        \"\"\"\n",
    "        Initializes the AdditionalFeatures extractor.\n",
    "\n",
    "        Args:\n",
    "            country (str): Country code for holiday detection (e.g., 'US', 'IL').\n",
    "        \"\"\"\n",
    "        self.holiday = holidays.CountryHoliday(country)\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self  # no fitting needed\n",
    "\n",
    "    def transform(self, X):\n",
    "        \"\"\"\n",
    "        Transforms tweet text and timestamp columns into numerical features.\n",
    "\n",
    "        Args:\n",
    "            X (pd.DataFrame or dict-like): Must contain 'tweet text' and 'time stamp' keys.\n",
    "\n",
    "        Returns:\n",
    "            np.ndarray: Feature matrix (n_samples x n_features)\n",
    "        \"\"\"\n",
    "        return np.array([\n",
    "            self.extract_features(text, timestamp)\n",
    "            for text, timestamp in zip(X['tweet text'], X['time stamp'])\n",
    "        ])\n",
    "\n",
    "    def extract_features(self, text, timestamp):\n",
    "        \"\"\"\n",
    "        Extracts features from a single tweet and timestamp.\n",
    "\n",
    "        Args:\n",
    "            text (str): The tweet text.\n",
    "            timestamp (str): Timestamp string in format '%Y-%m-%d %H:%M:%S'.\n",
    "\n",
    "        Returns:\n",
    "            list: A fixed-length feature vector.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # --- Text features ---\n",
    "            words = text.split()\n",
    "            num_words = len(words)\n",
    "            num_capitalized = sum(1 for w in words if w.isupper())\n",
    "            num_question_marks = text.count('?')\n",
    "            num_exclamations = text.count('!')\n",
    "            avg_word_length = np.mean([len(w) for w in words]) if words else 0\n",
    "            has_all_caps_word = int(any(w.isupper() and len(w) > 1 for w in words))\n",
    "\n",
    "            # --- Timestamp features ---\n",
    "            dt = datetime.strptime(timestamp, '%Y-%m-%d %H:%M:%S')\n",
    "            is_holiday = int(dt.date() in self.holiday)\n",
    "            hour = dt.hour\n",
    "            day_of_week = dt.weekday()  # Monday = 0\n",
    "            month = dt.month\n",
    "            year = dt.year\n",
    "\n",
    "            return [\n",
    "                num_words,\n",
    "                num_capitalized,\n",
    "                num_question_marks,\n",
    "                num_exclamations,\n",
    "                avg_word_length,\n",
    "                has_all_caps_word,\n",
    "                is_holiday,\n",
    "                hour,\n",
    "                day_of_week,\n",
    "                month,\n",
    "                year\n",
    "            ]\n",
    "\n",
    "        except Exception:\n",
    "            # Fallback: return default values\n",
    "            return [0] * 11\n",
    "\n",
    "class Word2VecVectorizer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, size=100, min_count=1, window=5, workers=4, sg=1):\n",
    "        \"\"\"\n",
    "        Word2Vec-based vectorizer.\n",
    "\n",
    "        Args:\n",
    "            size (int): Dimensionality of word vectors.\n",
    "            min_count (int): Minimum word frequency for training.\n",
    "            window (int): Context window size.\n",
    "            workers (int): Number of worker threads.\n",
    "            sg (int): Skip-gram (1) vs CBOW (0) mode.\n",
    "        \"\"\"\n",
    "        self.size = size\n",
    "        self.min_count = min_count\n",
    "        self.window = window\n",
    "        self.workers = workers\n",
    "        self.sg = sg\n",
    "        self.model = None\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        \"\"\"\n",
    "        Trains Word2Vec model on tokenized input texts.\n",
    "\n",
    "        Args:\n",
    "            X (list of str): Input texts.\n",
    "\n",
    "        Returns:\n",
    "            self\n",
    "        \"\"\"\n",
    "        tokenized = [text.split() for text in X]\n",
    "        self.model = Word2Vec(\n",
    "            sentences=tokenized,\n",
    "            vector_size=self.size,\n",
    "            window=self.window,\n",
    "            min_count=self.min_count,\n",
    "            workers=self.workers,\n",
    "            sg=self.sg\n",
    "        )\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        \"\"\"\n",
    "        Converts each text to an averaged Word2Vec embedding.\n",
    "\n",
    "        Args:\n",
    "            X (list of str): Input texts.\n",
    "\n",
    "        Returns:\n",
    "            np.ndarray: Matrix of shape (n_samples, size)\n",
    "        \"\"\"\n",
    "        vectors = []\n",
    "        for text in X:\n",
    "            tokens = self._tokenize(text)\n",
    "            vecs = [self.model.wv[word] for word in tokens if word in self.model.wv]\n",
    "            if vecs:\n",
    "                vectors.append(np.mean(vecs, axis=0))\n",
    "            else:\n",
    "                vectors.append(np.zeros(self.size))\n",
    "        return np.array(vectors)\n",
    "\n",
    "    def transform(self, X):\n",
    "        return np.array([self.vectorize(text) for text in X])\n",
    "\n",
    "    def vectorize(self, text):\n",
    "        words = text.split()\n",
    "        word_vecs = [self.model.wv[word] for word in words if word in self.model.wv]\n",
    "        if len(word_vecs) == 0:\n",
    "            return np.zeros(self.size)\n",
    "        return np.mean(word_vecs, axis=0)\n",
    "\n",
    "\n",
    "\n",
    "class FeatureVectorizer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, vector_types=['tfidf', 'word2vec', 'additional'], country='US',\n",
    "                 word2vec_size=100, tfidf_max_features=5000):\n",
    "        \"\"\"\n",
    "        Combines multiple feature vector types into a single feature matrix.\n",
    "\n",
    "        Args:\n",
    "            vector_types (list): Which vectors to use. Options: 'tfidf', 'word2vec', 'additional'.\n",
    "            country (str): Country code for holiday feature in AdditionalFeatures.\n",
    "            word2vec_size (int): Dimensionality of Word2Vec vectors.\n",
    "            tfidf_max_features (int): Max features for TF-IDF vectorizer.\n",
    "        \"\"\"\n",
    "        self.vector_types = vector_types\n",
    "        self.country = country\n",
    "        self.word2vec_size = word2vec_size\n",
    "        self.tfidf_max_features = tfidf_max_features\n",
    "        self.additional = AdditionalFeatures(country=self.country)\n",
    "        # Components (lazy initialization)\n",
    "        if 'tfidf' in self.vector_types:\n",
    "            self.tfidf = TfidfVectorizer(max_features=self.tfidf_max_features)\n",
    "        if 'word2vec' in self.vector_types:\n",
    "            self.word2vec = Word2VecVectorizer(size=self.word2vec_size)\n",
    "        if 'additional' in self.vector_types:\n",
    "            self.additional = AdditionalFeatures(country=self.country)\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        \"\"\"\n",
    "        Fits underlying vectorizers as needed.\n",
    "        \"\"\"\n",
    "        if 'tfidf' in self.vector_types:\n",
    "            self.tfidf.fit(X['cleaned text'])\n",
    "        if 'word2vec' in self.vector_types:\n",
    "            self.word2vec.fit(X['cleaned text'])\n",
    "        if 'additional' in self.vector_types:\n",
    "            self.additional.fit(X)\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        \"\"\"\n",
    "        Transforms input data into a combined feature matrix.\n",
    "        \"\"\"\n",
    "        features = []\n",
    "\n",
    "        if 'tfidf' in self.vector_types:\n",
    "            tfidf_vec = self.tfidf.transform(X['cleaned text']).toarray()\n",
    "            features.append(tfidf_vec)\n",
    "\n",
    "        if 'word2vec' in self.vector_types:\n",
    "            w2v_vec = self.word2vec.transform(X['cleaned text'])\n",
    "            features.append(w2v_vec)\n",
    "\n",
    "        if 'additional' in self.vector_types:\n",
    "            additional_vec = self.additional.transform(X)\n",
    "            features.append(additional_vec)\n",
    "\n",
    "        return np.concatenate(features, axis=1)\n",
    "\n",
    "    def fit_transform(self, X, y=None):\n",
    "        return self.fit(X, y).transform(X)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Qy428MPY1r__"
   },
   "source": [
    "**Algorithmic approaches:**\n",
    "\n",
    "\n",
    "\n",
    "1. [sklearn.linear_model.LogisticRegression](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html#sklearn.linear_model.LogisticRegression)\n",
    "2. [sklearn.svm.SVC](http://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html#sklearn.svm.SVC) (try both linear and nonlinear kernels!)\n",
    "3. FFNN classifier - You should use  the PyTorch library to build a FFNN classifier (with at least one hidden layer) to achieve the classification. Feel free to experiment with the number of layers ([a simple tutorial for FFNN with PyTorch](https://medium.com/biaslyai/pytorch-introduction-to-neural-network-feedforward-neural-network-model-e7231cff47cb)).\n",
    "4. A fourth classifier of choice (neural or not). You are encouraged to experiment with classifiers that allow combining different types of features (e.g. number of capitalized words, time of tweeting, etc.)\n",
    "5. A fifth classifier of your choice  (this should be neural -  RNN, or transformer-based) - feel free to experiment.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TewSUOTWNqqG"
   },
   "source": [
    "## Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "C8m8quKbI0e_"
   },
   "outputs": [],
   "source": [
    "class MultiLayerFFNN(nn.Module):\n",
    "    '''\n",
    "    Multilayer FFNN\n",
    "    '''\n",
    "    def __init__(self, input_dim, hidden_dims, output_dim=2, dropout=0.5):\n",
    "        super(MultiLayerFFNN, self).__init__()\n",
    "        layers = []\n",
    "        current_dim = input_dim\n",
    "\n",
    "        for hidden_dim in hidden_dims:\n",
    "            layers.append(nn.Linear(current_dim, hidden_dim))\n",
    "            layers.append(nn.ReLU())\n",
    "            layers.append(nn.Dropout(dropout))\n",
    "            current_dim = hidden_dim\n",
    "\n",
    "        layers.append(nn.Linear(current_dim, output_dim))\n",
    "        layers.append(nn.LogSoftmax(dim=1))\n",
    "\n",
    "        self.model = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "    def predict(self, input_data):\n",
    "        self.eval()  # Set the model to evaluation mode\n",
    "        with torch.no_grad():  # Disable gradient calculation\n",
    "            outputs = self(input_data)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "        return predicted\n",
    "\n",
    "\n",
    "class PyTorchClassifier(BaseEstimator, ClassifierMixin):\n",
    "    def __init__(self, hidden_dims=None, output_dim=2, dropout=0.3, learning_rate=0.001, print=False):\n",
    "        self.hidden_dims = hidden_dims if hidden_dims is not None else [50, 50]\n",
    "        self.output_dim = output_dim\n",
    "        self.dropout = dropout\n",
    "        self.learning_rate = learning_rate\n",
    "        self.num_epochs = len(self.hidden_dims) * 50\n",
    "        self.model = None\n",
    "        self.criterion = nn.NLLLoss()\n",
    "        self.optimizer = None\n",
    "        self.print = print\n",
    "\n",
    "    def set_params(self, **params):\n",
    "        for param, value in params.items():\n",
    "            setattr(self, param, value)\n",
    "        if 'hidden_dims' in params:\n",
    "            self.num_epochs = len(self.hidden_dims) * 50\n",
    "        return self\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        input_dim = X.shape[1]\n",
    "        self.model = MultiLayerFFNN(input_dim, self.hidden_dims, self.output_dim, self.dropout)\n",
    "        self.optimizer = optim.Adam(self.model.parameters(), lr=self.learning_rate)\n",
    "\n",
    "        X_tensor = torch.tensor(X, dtype=torch.float32)\n",
    "        y_tensor = torch.tensor(y.to_numpy(), dtype=torch.long)\n",
    "\n",
    "        dataset = TensorDataset(X_tensor, y_tensor)\n",
    "        train_loader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "        for epoch in range(self.num_epochs):\n",
    "            self.model.train()\n",
    "            running_loss = 0.0\n",
    "            for inputs, labels in train_loader:\n",
    "                self.optimizer.zero_grad()\n",
    "                outputs = self.model(inputs)\n",
    "                loss = self.criterion(outputs, labels)\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "                running_loss += loss.item()\n",
    "            self.print and print(f\"Epoch [{epoch+1}/{self.num_epochs}], Loss: {running_loss/len(train_loader):.4f}\")\n",
    "\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        X_tensor = torch.tensor(X, dtype=torch.float32)\n",
    "        predictions = self.model.predict(X_tensor)\n",
    "        return predictions.numpy()\n",
    "\n",
    "\n",
    "\n",
    "class BertHFModel:\n",
    "    def __init__(\n",
    "        self,\n",
    "        text_col: str = \"cleaned text\",\n",
    "        model_name: str = \"prajjwal1/bert-mini\",\n",
    "        max_len: int = 64,\n",
    "        epochs: int = 4,\n",
    "        batch_size: int = 8,\n",
    "        lr: float = 3e-5,\n",
    "        warmup_ratio: float = 0.1,\n",
    "        weight_decay: float = 0.01,\n",
    "        unfreeze_last_n: int = 2,\n",
    "        fp16: bool | None = None,\n",
    "        output_dir: str = \"fast_bert_plus_ckpt\",\n",
    "        verbose: bool = True,\n",
    "    ):\n",
    "        self.text_col = text_col\n",
    "        self.max_len = max_len\n",
    "        self.epochs = epochs\n",
    "        self.batch = batch_size\n",
    "        self.lr = lr\n",
    "        self.warmup = warmup_ratio\n",
    "        self.wd = weight_decay\n",
    "        self.out_dir = output_dir\n",
    "        self.verbose = verbose\n",
    "        self.fp16 = torch.cuda.is_available() if fp16 is None else fp16\n",
    "        self.unfreeze = unfreeze_last_n\n",
    "\n",
    "        self.tok = BertTokenizerFast.from_pretrained(model_name)\n",
    "        self.model = BertForSequenceClassification.from_pretrained(\n",
    "            model_name, num_labels=2\n",
    "        )\n",
    "\n",
    "        # Unfreeze last N encoder layers + classifier + LayerNorm\n",
    "        enc_layers = list(self.model.bert.encoder.layer)\n",
    "        for name, p in self.model.named_parameters():\n",
    "            p.requires_grad = (\n",
    "                name.startswith(\"classifier\")\n",
    "                or any(name.startswith(f\"bert.encoder.layer.{i}\") for i in range(len(enc_layers) - self.unfreeze, len(enc_layers)))\n",
    "                or \"LayerNorm\" in name\n",
    "            )\n",
    "\n",
    "        self.trainer: Trainer | None = None\n",
    "\n",
    "    @staticmethod\n",
    "    def _make_args(out_dir: str, **kw):\n",
    "        import inspect\n",
    "        TA = TrainingArguments\n",
    "        sig = inspect.signature(TA.__init__).parameters\n",
    "        filt = {k: v for k, v in kw.items() if k in sig}\n",
    "        if \"evaluation_strategy\" not in sig or \"save_strategy\" not in sig:\n",
    "            filt.pop(\"load_best_model_at_end\", None)\n",
    "            filt.pop(\"metric_for_best_model\", None)\n",
    "        return TA(out_dir, **filt)\n",
    "\n",
    "    def train(self, df: pd.DataFrame, label_col=\"author\"):\n",
    "        ds = Dataset.from_pandas(df[[self.text_col, label_col]])\n",
    "        tok_ds = (\n",
    "            ds.map(\n",
    "                lambda e: self.tok(\n",
    "                    e[self.text_col],\n",
    "                    padding=\"max_length\",\n",
    "                    truncation=True,\n",
    "                    max_length=self.max_len,\n",
    "                ),\n",
    "                batched=True,\n",
    "            )\n",
    "            .rename_column(label_col, \"labels\")\n",
    "            .remove_columns([self.text_col])\n",
    "        )\n",
    "        tok_ds.set_format(\"torch\")\n",
    "\n",
    "        y = np.array(tok_ds[\"labels\"])\n",
    "        idx_tr, idx_val = train_test_split(np.arange(len(y)), test_size=0.1, stratify=y, random_state=42)\n",
    "        train_ds, val_ds = tok_ds.select(idx_tr), tok_ds.select(idx_val)\n",
    "\n",
    "        args = self._make_args(\n",
    "            self.out_dir,\n",
    "            per_device_train_batch_size=self.batch,\n",
    "            per_device_eval_batch_size=self.batch,\n",
    "            num_train_epochs=self.epochs,\n",
    "            learning_rate=self.lr,\n",
    "            warmup_ratio=self.warmup,\n",
    "            weight_decay=self.wd,\n",
    "            fp16=self.fp16,\n",
    "            evaluation_strategy=\"epoch\",\n",
    "            save_strategy=\"epoch\",\n",
    "            logging_strategy=\"epoch\",\n",
    "            load_best_model_at_end=True,\n",
    "            metric_for_best_model=\"accuracy\",\n",
    "            report_to=\"none\",\n",
    "        )\n",
    "\n",
    "        def metrics(pred):\n",
    "            logit, label = pred\n",
    "            p = logit.argmax(-1)\n",
    "            acc = accuracy_score(label, p)\n",
    "            prec, rec, f1, _ = precision_recall_fscore_support(label, p, average=\"binary\")\n",
    "            return {\"accuracy\": acc, \"precision\": prec, \"recall\": rec, \"f1\": f1}\n",
    "\n",
    "        self.trainer = Trainer(\n",
    "            model=self.model,\n",
    "            args=args,\n",
    "            train_dataset=train_ds,\n",
    "            eval_dataset=val_ds,\n",
    "            tokenizer=self.tok,\n",
    "            data_collator=DataCollatorWithPadding(self.tok),\n",
    "            compute_metrics=metrics,\n",
    "        )\n",
    "\n",
    "        if self.verbose:\n",
    "            print(\"⚡ Fine-tuning BERT-mini (last-2 layers) …\")\n",
    "        self.trainer.train()\n",
    "\n",
    "        best_acc = getattr(self.trainer.state, \"best_metric\", None)\n",
    "        best_step = getattr(self.trainer.state, \"best_step\", None)\n",
    "        if best_acc is not None and best_step is not None:\n",
    "            steps_per_epoch = math.ceil(len(train_ds) / self.batch)\n",
    "            best_epoch = int(round(best_step / steps_per_epoch))\n",
    "            best_params = {\n",
    "                \"lr\": self.lr,\n",
    "                \"unfreeze_last\": self.unfreeze,\n",
    "                \"max_len\": self.max_len,\n",
    "                \"batch_size\": self.batch,\n",
    "            }\n",
    "            print(f\"Best Epoch #{best_epoch}: Accuracy={best_acc:.4f} | Params={best_params}\")\n",
    "\n",
    "    def predict(self, df: pd.DataFrame):\n",
    "        if self.trainer is None:\n",
    "            raise ValueError(\"Call train() first.\")\n",
    "        ds = Dataset.from_pandas(df[[self.text_col]])\n",
    "        ds = ds.map(\n",
    "            lambda e: self.tok(\n",
    "                e[self.text_col],\n",
    "                padding=\"max_length\",\n",
    "                truncation=True,\n",
    "                max_length=self.max_len,\n",
    "            ),\n",
    "            batched=True,\n",
    "        ).remove_columns([self.text_col])\n",
    "        ds.set_format(\"torch\")\n",
    "        logits = self.trainer.predict(ds).predictions\n",
    "        return logits.argmax(-1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mA3QXh7Uyn9l"
   },
   "source": [
    "##API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JMEN3rBq1_wL"
   },
   "source": [
    "Your notebook should support the following functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3GILqQTF-eNg",
    "outputId": "0e2ef264-8a4d-42c2-f3f1-c756334be5e5"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
      "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
      "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
      "You will be able to reuse this secret in all of your notebooks.\n",
      "Please note that authentication is recommended but still optional to access public models or datasets.\n",
      "  warnings.warn(\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at prajjwal1/bert-mini and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "hyperparameters = {\n",
    "    1: {\n",
    "        'model_name': 'LogisticRegression',\n",
    "        'vector': ['tfidf'],\n",
    "        'best_hyperparams': {\n",
    "           'C': 9.864948922299297,\n",
    "        }\n",
    "\n",
    "\n",
    "    },\n",
    "    2: {\n",
    "        'model_name': 'SVC',\n",
    "        'vector': ['tfidf', 'additional'],\n",
    "        'best_hyperparams': {\n",
    "            'kernel': 'rbf',\n",
    "            'C': 4.31289165277942,\n",
    "            'gamma': 0.030396135757067042\n",
    "        }\n",
    "    },\n",
    "    3: {\n",
    "        'model_name': 'XGBClassifier',\n",
    "        'vector': ['tfidf', 'additional'],\n",
    "        'best_hyperparams': {\n",
    "            'n_estimators': 477,\n",
    "            'max_depth': 9,\n",
    "            'learning_rate': 0.016548545657454578,\n",
    "            'subsample': 0.8444703426347393,\n",
    "            'colsample_bytree': 0.6238505376857482,\n",
    "            'reg_lambda': 0.4156047528735853\n",
    "        }\n",
    "    },\n",
    "    4: {\n",
    "        'model_name': 'FFNN',\n",
    "        'vector': ['tfidf'],\n",
    "        'best_hyperparams': {\n",
    "            'hidden': [50],\n",
    "        }\n",
    "    },\n",
    "    5: {\n",
    "        'model_name': 'BERT',\n",
    "        'vector': None,\n",
    "        'best_hyperparams': None\n",
    "\n",
    "\n",
    "    }\n",
    "}\n",
    "\n",
    "models = {\n",
    "    1: LogisticRegression(max_iter=1000, **hyperparameters[1]['best_hyperparams']),\n",
    "    2: SVC(**hyperparameters[2]['best_hyperparams']),\n",
    "    3: XGBClassifier(use_label_encoder=False, eval_metric='mlogloss', **hyperparameters[3]['best_hyperparams']),\n",
    "    4: PyTorchClassifier(hidden_dims=hyperparameters[4]['best_hyperparams']['hidden']),\n",
    "    5: BertHFModel()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "lQTjw-TpgV_V"
   },
   "outputs": [],
   "source": [
    "def training_pipeline(alg, train_fn='trump_train.tsv', test=True, train=False):\n",
    "    \"\"\"\n",
    "      Trains a model (1=LR, 2=SVM, 3=FFNN, 4=PyTorch FFNN, 5=BERT) with preprocessing and evaluation.\n",
    "\n",
    "      Args:\n",
    "          alg (int): Algorithm ID.\n",
    "          train_fn (str): Path to training file.\n",
    "          test (bool): Whether to split for evaluation.\n",
    "          train (bool): Whether to train or use fixed hyperparameters.\n",
    "\n",
    "      Returns:\n",
    "          Trained pipeline or BERT model.\n",
    "    \"\"\"\n",
    "\n",
    "    column_names = ['tweet id', 'user handle', 'tweet text', 'time stamp', 'device']\n",
    "    df = pd.read_csv(train_fn, sep='\\t', names=column_names, header=None, quoting=csv.QUOTE_NONE)\n",
    "\n",
    "    print(\"🔄 Preprocessing...\")\n",
    "    df['cleaned text'] = df['tweet text'].apply(Preprocesstext)\n",
    "    avg_ts = average_timestamp(df[\"time stamp\"])\n",
    "    df[\"time stamp\"] = df[\"time stamp\"].apply(lambda ts: ts if is_valid_timestamp(ts) else avg_ts)\n",
    "    df[\"author\"] = df.apply(lambda row: classify_tweet(row[\"device\"], row[\"user handle\"], row[\"time stamp\"]), axis=1)\n",
    "    df.dropna(subset=[\"author\"], inplace=True)\n",
    "    df[\"author\"] = df[\"author\"].astype(int)\n",
    "\n",
    "    if test:\n",
    "        df_train, df_test = train_test_split(df, test_size=0.2, stratify=df[\"author\"], random_state=42)\n",
    "    else:\n",
    "        df_train, df_test = df, None\n",
    "\n",
    "    if alg < 5:\n",
    "        if train:\n",
    "            print(\"🧪 Training with all vectorizer combinations and cross-validation...\")\n",
    "\n",
    "            base_vectors = ['tfidf', 'word2vec']\n",
    "            feature_combinations = [\n",
    "                [bv] for bv in base_vectors\n",
    "            ] + [\n",
    "                [bv, 'additional'] for bv in base_vectors\n",
    "            ]\n",
    "\n",
    "            best_score = 0\n",
    "            best_vector_types = None\n",
    "            best_model = None\n",
    "            trial = 1\n",
    "\n",
    "            for vt in feature_combinations:\n",
    "                print(f\"\\n🔁 Trial #{trial}\")\n",
    "                print(f\"🔢 Vector Types: {vt}\")\n",
    "                model = models[alg]\n",
    "                print(f\"🧪 Hyperparameters: {model.get_params()}\")\n",
    "\n",
    "                pipeline = Pipeline([\n",
    "                    ('features', FeatureVectorizer(vector_types=vt, country='US')),\n",
    "                    ('classifier', model)\n",
    "                ])\n",
    "\n",
    "                scores = cross_val_score(pipeline, df_train.drop(columns=['author']), df_train['author'], cv=3, scoring='f1')\n",
    "                avg_score = scores.mean()\n",
    "                print(f\"📉 F1 scores per fold: {scores}\")\n",
    "                print(f\"📈 Avg F1-score: {avg_score:.4f}\")\n",
    "\n",
    "                if avg_score > best_score:\n",
    "                    best_score = avg_score\n",
    "                    best_vector_types = vt\n",
    "                    best_model = model\n",
    "\n",
    "                trial += 1\n",
    "\n",
    "            print(\"\\n🏁 Final Best Configuration\")\n",
    "            print(f\"✅ Vector Types: {best_vector_types}\")\n",
    "            print(f\"✅ F1 Score: {best_score:.4f}\")\n",
    "            print(f\"✅ Hyperparameters: {best_model.get_params()}\")\n",
    "\n",
    "            pipeline = Pipeline([\n",
    "                ('features', FeatureVectorizer(vector_types=best_vector_types, country='US')),\n",
    "                ('classifier', best_model)\n",
    "            ])\n",
    "            pipeline.fit(df_train.drop(columns=['author']), df_train['author'])\n",
    "\n",
    "        else:\n",
    "            vector_types = hyperparameters[alg]['vector']\n",
    "            print(f\"📦 Loaded vector types from config: {vector_types}\")\n",
    "\n",
    "            model = models[alg]\n",
    "            pipeline = Pipeline([\n",
    "                ('features', FeatureVectorizer(vector_types=vector_types, country='US')),\n",
    "                ('classifier', model)\n",
    "            ])\n",
    "            pipeline.fit(df_train.drop(columns=['author']), df_train['author'])\n",
    "\n",
    "    else:\n",
    "        model = BertHFModel()\n",
    "        model.train(df_train, label_col=\"author\")\n",
    "        pipeline = model\n",
    "\n",
    "    if test:\n",
    "        X_test = df_test.drop(columns=['author'])\n",
    "        y_test = df_test['author']\n",
    "        y_pred = pipeline.predict(X_test)\n",
    "\n",
    "        print(\"\\n🧪 Evaluation on Test Set\")\n",
    "        print(classification_report(y_test, y_pred))\n",
    "        print(f\"Accuracy : {accuracy_score(y_test, y_pred):.2f}\")\n",
    "        print(f\"F1 Score : {f1_score(y_test, y_pred):.2f}\")\n",
    "        print(f\"Recall   : {recall_score(y_test, y_pred):.2f}\")\n",
    "        print(f\"Precision: {precision_score(y_test, y_pred):.2f}\")\n",
    "\n",
    "    return pipeline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "fXYZRHxK2MMu"
   },
   "outputs": [],
   "source": [
    "def retrain_best_model():\n",
    "  \"\"\" Retrains and returns the best performing model for the specified task. The\n",
    "      function uses the hard coded settings you have found to work best for each\n",
    "      of the tasks.\n",
    "\n",
    "      Args:\n",
    "\n",
    "  \"\"\"\n",
    "  # Create a pipeline with the custom feature vectorizer and the classifier\n",
    "  m = training_pipeline(3, test = False)\n",
    "\n",
    "  return m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "L6L0ZFSQ2Wjf"
   },
   "outputs": [],
   "source": [
    "def predict(pipeline, fn):\n",
    "    \"\"\" Returns a list of 0s and 1s, corresponding to the lines in the specified file.\n",
    "\n",
    "    Args:\n",
    "        pipeline: the trained model and process to be used.\n",
    "        fn: the full path to a file in the same format as the test set we have provided.\n",
    "    \"\"\"\n",
    "    print('🔄 Pre-Process Data')\n",
    "    # Define the column names\n",
    "    column_names = ['user handle', 'tweet text', 'time stamp']\n",
    "\n",
    "    # Load the TSV file into a DataFrame with specified column names\n",
    "    df = pd.read_csv(fn, sep='\\t', names=column_names, header=None, quoting=csv.QUOTE_NONE)\n",
    "\n",
    "    # Process the Text\n",
    "    df['cleaned text'] = df['tweet text'].apply(Preprocesstext)\n",
    "    avg_timestamp = average_timestamp(df['time stamp'])\n",
    "    df[\"time stamp\"] = df[\"time stamp\"].apply(lambda ts: ts if is_valid_timestamp(ts) else avg_timestamp)\n",
    "\n",
    "    print('📊 Predicting...')\n",
    "    if isinstance(pipeline, Pipeline):\n",
    "        predictions = pipeline.predict(df)\n",
    "    elif hasattr(pipeline, \"predict\"):\n",
    "        predictions = pipeline.predict(df)  # this is your BertHFModel\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported model type for prediction\")\n",
    "\n",
    "    return predictions.tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lXtCY3Mc2c4q"
   },
   "outputs": [],
   "source": [
    "def who_am_i():  # this is not a class method\n",
    "    \"\"\"Returns a list of dictionaries, each dictionary with your name, id number and email. keys=['name', 'id','email']\n",
    "        (If you are submitting solo, the list should contain only one dictionary. If you submit as a team, the list should\n",
    "        contain a dictionary for each team member.)\n",
    "        Make sure you return your own info!\n",
    "    \"\"\"\n",
    "    return [{'name': 'Nir Rahav', 'id': '316275437', 'email': 'rahavn@post.bgu.ac.il'}, {'name': 'Yael Berkovich', 'id': '324879501', 'email': 'berkova@post.bgu.ac.il'}]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HxzyEnu4UeED"
   },
   "source": [
    "## Local testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "ZMuIiMA1Uj2f"
   },
   "outputs": [],
   "source": [
    "train_fn = '/content/trump_train.tsv'\n",
    "test_fn = '/content/trump_tweets_test_a.tsv'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qzwg1VjQUocA",
    "outputId": "3967a447-a805-4466-e6dc-0d11d542fcf8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔄 Preprocessing...\n",
      "📦 Loaded vector types from config: ['tfidf']\n",
      "\n",
      "🧪 Evaluation on Test Set\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.94      0.90       451\n",
      "           1       0.87      0.73      0.79       255\n",
      "\n",
      "    accuracy                           0.86       706\n",
      "   macro avg       0.86      0.83      0.84       706\n",
      "weighted avg       0.86      0.86      0.86       706\n",
      "\n",
      "Accuracy : 0.86\n",
      "F1 Score : 0.79\n",
      "Recall   : 0.73\n",
      "Precision: 0.87\n"
     ]
    }
   ],
   "source": [
    "model = training_pipeline(1, train_fn, test = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "U3UOSnveUo9p",
    "outputId": "67832238-1dcc-4ca5-8a9e-0dd7ba98d96b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔄 Preprocessing...\n",
      "🧪 Training with all vectorizer combinations and cross-validation...\n",
      "\n",
      "🔁 Trial #1\n",
      "🔢 Vector Types: ['tfidf']\n",
      "🧪 Hyperparameters: {'C': 4.31289165277942, 'break_ties': False, 'cache_size': 200, 'class_weight': None, 'coef0': 0.0, 'decision_function_shape': 'ovr', 'degree': 3, 'gamma': 0.030396135757067042, 'kernel': 'rbf', 'max_iter': -1, 'probability': False, 'random_state': None, 'shrinking': True, 'tol': 0.001, 'verbose': False}\n",
      "📉 F1 scores per fold: [0.66159696 0.68805704 0.69776119]\n",
      "📈 Avg F1-score: 0.6825\n",
      "\n",
      "🔁 Trial #2\n",
      "🔢 Vector Types: ['word2vec']\n",
      "🧪 Hyperparameters: {'C': 4.31289165277942, 'break_ties': False, 'cache_size': 200, 'class_weight': None, 'coef0': 0.0, 'decision_function_shape': 'ovr', 'degree': 3, 'gamma': 0.030396135757067042, 'kernel': 'rbf', 'max_iter': -1, 'probability': False, 'random_state': None, 'shrinking': True, 'tol': 0.001, 'verbose': False}\n",
      "📉 F1 scores per fold: [0.07282913 0.0509915  0.        ]\n",
      "📈 Avg F1-score: 0.0413\n",
      "\n",
      "🔁 Trial #3\n",
      "🔢 Vector Types: ['tfidf', 'additional']\n",
      "🧪 Hyperparameters: {'C': 4.31289165277942, 'break_ties': False, 'cache_size': 200, 'class_weight': None, 'coef0': 0.0, 'decision_function_shape': 'ovr', 'degree': 3, 'gamma': 0.030396135757067042, 'kernel': 'rbf', 'max_iter': -1, 'probability': False, 'random_state': None, 'shrinking': True, 'tol': 0.001, 'verbose': False}\n",
      "📉 F1 scores per fold: [0.75       0.734375   0.74570983]\n",
      "📈 Avg F1-score: 0.7434\n",
      "\n",
      "🔁 Trial #4\n",
      "🔢 Vector Types: ['word2vec', 'additional']\n",
      "🧪 Hyperparameters: {'C': 4.31289165277942, 'break_ties': False, 'cache_size': 200, 'class_weight': None, 'coef0': 0.0, 'decision_function_shape': 'ovr', 'degree': 3, 'gamma': 0.030396135757067042, 'kernel': 'rbf', 'max_iter': -1, 'probability': False, 'random_state': None, 'shrinking': True, 'tol': 0.001, 'verbose': False}\n",
      "📉 F1 scores per fold: [0.74164134 0.71721959 0.74015748]\n",
      "📈 Avg F1-score: 0.7330\n",
      "\n",
      "🏁 Final Best Configuration\n",
      "✅ Vector Types: ['tfidf', 'additional']\n",
      "✅ F1 Score: 0.7434\n",
      "✅ Hyperparameters: {'C': 4.31289165277942, 'break_ties': False, 'cache_size': 200, 'class_weight': None, 'coef0': 0.0, 'decision_function_shape': 'ovr', 'degree': 3, 'gamma': 0.030396135757067042, 'kernel': 'rbf', 'max_iter': -1, 'probability': False, 'random_state': None, 'shrinking': True, 'tol': 0.001, 'verbose': False}\n",
      "\n",
      "🧪 Evaluation on Test Set\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      0.90      0.86       451\n",
      "           1       0.79      0.69      0.73       255\n",
      "\n",
      "    accuracy                           0.82       706\n",
      "   macro avg       0.81      0.79      0.80       706\n",
      "weighted avg       0.82      0.82      0.82       706\n",
      "\n",
      "Accuracy : 0.82\n",
      "F1 Score : 0.73\n",
      "Recall   : 0.69\n",
      "Precision: 0.79\n"
     ]
    }
   ],
   "source": [
    "model = training_pipeline(2, train_fn, test = True, train = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Na9pOPYUUqNw",
    "outputId": "dcd587e2-bbef-4245-fca0-7ba7963eadbf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔄 Preprocessing...\n",
      "🧪 Training with all vectorizer combinations and cross-validation...\n",
      "\n",
      "🔁 Trial #1\n",
      "🔢 Vector Types: ['tfidf']\n",
      "🧪 Hyperparameters: {'objective': 'binary:logistic', 'base_score': None, 'booster': None, 'callbacks': None, 'colsample_bylevel': None, 'colsample_bynode': None, 'colsample_bytree': 0.6238505376857482, 'device': None, 'early_stopping_rounds': None, 'enable_categorical': False, 'eval_metric': 'mlogloss', 'feature_types': None, 'gamma': None, 'grow_policy': None, 'importance_type': None, 'interaction_constraints': None, 'learning_rate': 0.016548545657454578, 'max_bin': None, 'max_cat_threshold': None, 'max_cat_to_onehot': None, 'max_delta_step': None, 'max_depth': 9, 'max_leaves': None, 'min_child_weight': None, 'missing': nan, 'monotone_constraints': None, 'multi_strategy': None, 'n_estimators': 477, 'n_jobs': None, 'num_parallel_tree': None, 'random_state': None, 'reg_alpha': None, 'reg_lambda': 0.4156047528735853, 'sampling_method': None, 'scale_pos_weight': None, 'subsample': 0.8444703426347393, 'tree_method': None, 'validate_parameters': None, 'verbosity': None, 'use_label_encoder': False}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [06:34:28] WARNING: /workspace/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [06:34:48] WARNING: /workspace/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [06:35:00] WARNING: /workspace/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📉 F1 scores per fold: [0.76872964 0.76507937 0.77408638]\n",
      "📈 Avg F1-score: 0.7693\n",
      "\n",
      "🔁 Trial #2\n",
      "🔢 Vector Types: ['word2vec']\n",
      "🧪 Hyperparameters: {'objective': 'binary:logistic', 'base_score': None, 'booster': None, 'callbacks': None, 'colsample_bylevel': None, 'colsample_bynode': None, 'colsample_bytree': 0.6238505376857482, 'device': None, 'early_stopping_rounds': None, 'enable_categorical': False, 'eval_metric': 'mlogloss', 'feature_types': None, 'gamma': None, 'grow_policy': None, 'importance_type': None, 'interaction_constraints': None, 'learning_rate': 0.016548545657454578, 'max_bin': None, 'max_cat_threshold': None, 'max_cat_to_onehot': None, 'max_delta_step': None, 'max_depth': 9, 'max_leaves': None, 'min_child_weight': None, 'missing': nan, 'monotone_constraints': None, 'multi_strategy': None, 'n_estimators': 477, 'n_jobs': None, 'num_parallel_tree': None, 'random_state': None, 'reg_alpha': None, 'reg_lambda': 0.4156047528735853, 'sampling_method': None, 'scale_pos_weight': None, 'subsample': 0.8444703426347393, 'tree_method': None, 'validate_parameters': None, 'verbosity': None, 'use_label_encoder': False}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [06:35:11] WARNING: /workspace/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [06:35:22] WARNING: /workspace/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [06:35:31] WARNING: /workspace/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📉 F1 scores per fold: [0.70546737 0.74662162 0.7137931 ]\n",
      "📈 Avg F1-score: 0.7220\n",
      "\n",
      "🔁 Trial #3\n",
      "🔢 Vector Types: ['tfidf', 'additional']\n",
      "🧪 Hyperparameters: {'objective': 'binary:logistic', 'base_score': None, 'booster': None, 'callbacks': None, 'colsample_bylevel': None, 'colsample_bynode': None, 'colsample_bytree': 0.6238505376857482, 'device': None, 'early_stopping_rounds': None, 'enable_categorical': False, 'eval_metric': 'mlogloss', 'feature_types': None, 'gamma': None, 'grow_policy': None, 'importance_type': None, 'interaction_constraints': None, 'learning_rate': 0.016548545657454578, 'max_bin': None, 'max_cat_threshold': None, 'max_cat_to_onehot': None, 'max_delta_step': None, 'max_depth': 9, 'max_leaves': None, 'min_child_weight': None, 'missing': nan, 'monotone_constraints': None, 'multi_strategy': None, 'n_estimators': 477, 'n_jobs': None, 'num_parallel_tree': None, 'random_state': None, 'reg_alpha': None, 'reg_lambda': 0.4156047528735853, 'sampling_method': None, 'scale_pos_weight': None, 'subsample': 0.8444703426347393, 'tree_method': None, 'validate_parameters': None, 'verbosity': None, 'use_label_encoder': False}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [06:35:41] WARNING: /workspace/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [06:35:57] WARNING: /workspace/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [06:36:14] WARNING: /workspace/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📉 F1 scores per fold: [0.83256528 0.82098765 0.84745763]\n",
      "📈 Avg F1-score: 0.8337\n",
      "\n",
      "🔁 Trial #4\n",
      "🔢 Vector Types: ['word2vec', 'additional']\n",
      "🧪 Hyperparameters: {'objective': 'binary:logistic', 'base_score': None, 'booster': None, 'callbacks': None, 'colsample_bylevel': None, 'colsample_bynode': None, 'colsample_bytree': 0.6238505376857482, 'device': None, 'early_stopping_rounds': None, 'enable_categorical': False, 'eval_metric': 'mlogloss', 'feature_types': None, 'gamma': None, 'grow_policy': None, 'importance_type': None, 'interaction_constraints': None, 'learning_rate': 0.016548545657454578, 'max_bin': None, 'max_cat_threshold': None, 'max_cat_to_onehot': None, 'max_delta_step': None, 'max_depth': 9, 'max_leaves': None, 'min_child_weight': None, 'missing': nan, 'monotone_constraints': None, 'multi_strategy': None, 'n_estimators': 477, 'n_jobs': None, 'num_parallel_tree': None, 'random_state': None, 'reg_alpha': None, 'reg_lambda': 0.4156047528735853, 'sampling_method': None, 'scale_pos_weight': None, 'subsample': 0.8444703426347393, 'tree_method': None, 'validate_parameters': None, 'verbosity': None, 'use_label_encoder': False}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [06:36:30] WARNING: /workspace/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [06:36:42] WARNING: /workspace/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [06:36:53] WARNING: /workspace/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📉 F1 scores per fold: [0.76948052 0.81037277 0.75693312]\n",
      "📈 Avg F1-score: 0.7789\n",
      "\n",
      "🏁 Final Best Configuration\n",
      "✅ Vector Types: ['tfidf', 'additional']\n",
      "✅ F1 Score: 0.8337\n",
      "✅ Hyperparameters: {'objective': 'binary:logistic', 'base_score': None, 'booster': None, 'callbacks': None, 'colsample_bylevel': None, 'colsample_bynode': None, 'colsample_bytree': 0.6238505376857482, 'device': None, 'early_stopping_rounds': None, 'enable_categorical': False, 'eval_metric': 'mlogloss', 'feature_types': None, 'gamma': None, 'grow_policy': None, 'importance_type': None, 'interaction_constraints': None, 'learning_rate': 0.016548545657454578, 'max_bin': None, 'max_cat_threshold': None, 'max_cat_to_onehot': None, 'max_delta_step': None, 'max_depth': 9, 'max_leaves': None, 'min_child_weight': None, 'missing': nan, 'monotone_constraints': None, 'multi_strategy': None, 'n_estimators': 477, 'n_jobs': None, 'num_parallel_tree': None, 'random_state': None, 'reg_alpha': None, 'reg_lambda': 0.4156047528735853, 'sampling_method': None, 'scale_pos_weight': None, 'subsample': 0.8444703426347393, 'tree_method': None, 'validate_parameters': None, 'verbosity': None, 'use_label_encoder': False}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [06:37:05] WARNING: /workspace/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🧪 Evaluation on Test Set\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.94      0.91       451\n",
      "           1       0.88      0.78      0.83       255\n",
      "\n",
      "    accuracy                           0.89       706\n",
      "   macro avg       0.89      0.86      0.87       706\n",
      "weighted avg       0.89      0.89      0.88       706\n",
      "\n",
      "Accuracy : 0.89\n",
      "F1 Score : 0.83\n",
      "Recall   : 0.78\n",
      "Precision: 0.88\n"
     ]
    }
   ],
   "source": [
    "model = training_pipeline(3, train_fn, test = True, train = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GLS3otRnUpvx",
    "outputId": "c10c1a2f-e63d-4391-c706-b6b1aa197e44"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔄 Preprocessing...\n",
      "🧪 Training with all vectorizer combinations and cross-validation...\n",
      "\n",
      "🔁 Trial #1\n",
      "🔢 Vector Types: ['tfidf']\n",
      "🧪 Hyperparameters: {'dropout': 0.3, 'hidden_dims': [50], 'learning_rate': 0.001, 'output_dim': 2, 'print': False}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/sklearn/pipeline.py:62: FutureWarning: This Pipeline instance is not fitted yet. Call 'fit' with appropriate arguments before using other methods such as transform, predict, etc. This will raise an error in 1.8 instead of the current warning.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/pipeline.py:62: FutureWarning: This Pipeline instance is not fitted yet. Call 'fit' with appropriate arguments before using other methods such as transform, predict, etc. This will raise an error in 1.8 instead of the current warning.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/pipeline.py:62: FutureWarning: This Pipeline instance is not fitted yet. Call 'fit' with appropriate arguments before using other methods such as transform, predict, etc. This will raise an error in 1.8 instead of the current warning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📉 F1 scores per fold: [0.71007752 0.70679012 0.7195122 ]\n",
      "📈 Avg F1-score: 0.7121\n",
      "\n",
      "🔁 Trial #2\n",
      "🔢 Vector Types: ['word2vec']\n",
      "🧪 Hyperparameters: {'dropout': 0.3, 'hidden_dims': [50], 'learning_rate': 0.001, 'output_dim': 2, 'print': False}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/sklearn/pipeline.py:62: FutureWarning: This Pipeline instance is not fitted yet. Call 'fit' with appropriate arguments before using other methods such as transform, predict, etc. This will raise an error in 1.8 instead of the current warning.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/pipeline.py:62: FutureWarning: This Pipeline instance is not fitted yet. Call 'fit' with appropriate arguments before using other methods such as transform, predict, etc. This will raise an error in 1.8 instead of the current warning.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/pipeline.py:62: FutureWarning: This Pipeline instance is not fitted yet. Call 'fit' with appropriate arguments before using other methods such as transform, predict, etc. This will raise an error in 1.8 instead of the current warning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📉 F1 scores per fold: [0.6996337  0.67030965 0.72277228]\n",
      "📈 Avg F1-score: 0.6976\n",
      "\n",
      "🔁 Trial #3\n",
      "🔢 Vector Types: ['tfidf', 'additional']\n",
      "🧪 Hyperparameters: {'dropout': 0.3, 'hidden_dims': [50], 'learning_rate': 0.001, 'output_dim': 2, 'print': False}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/sklearn/pipeline.py:62: FutureWarning: This Pipeline instance is not fitted yet. Call 'fit' with appropriate arguments before using other methods such as transform, predict, etc. This will raise an error in 1.8 instead of the current warning.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/pipeline.py:62: FutureWarning: This Pipeline instance is not fitted yet. Call 'fit' with appropriate arguments before using other methods such as transform, predict, etc. This will raise an error in 1.8 instead of the current warning.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/pipeline.py:62: FutureWarning: This Pipeline instance is not fitted yet. Call 'fit' with appropriate arguments before using other methods such as transform, predict, etc. This will raise an error in 1.8 instead of the current warning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📉 F1 scores per fold: [0.59491194 0.         0.58027079]\n",
      "📈 Avg F1-score: 0.3917\n",
      "\n",
      "🔁 Trial #4\n",
      "🔢 Vector Types: ['word2vec', 'additional']\n",
      "🧪 Hyperparameters: {'dropout': 0.3, 'hidden_dims': [50], 'learning_rate': 0.001, 'output_dim': 2, 'print': False}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/sklearn/pipeline.py:62: FutureWarning: This Pipeline instance is not fitted yet. Call 'fit' with appropriate arguments before using other methods such as transform, predict, etc. This will raise an error in 1.8 instead of the current warning.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/pipeline.py:62: FutureWarning: This Pipeline instance is not fitted yet. Call 'fit' with appropriate arguments before using other methods such as transform, predict, etc. This will raise an error in 1.8 instead of the current warning.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/pipeline.py:62: FutureWarning: This Pipeline instance is not fitted yet. Call 'fit' with appropriate arguments before using other methods such as transform, predict, etc. This will raise an error in 1.8 instead of the current warning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📉 F1 scores per fold: [0.39277652 0.57644991 0.61056106]\n",
      "📈 Avg F1-score: 0.5266\n",
      "\n",
      "🏁 Final Best Configuration\n",
      "✅ Vector Types: ['tfidf']\n",
      "✅ F1 Score: 0.7121\n",
      "✅ Hyperparameters: {'dropout': 0.3, 'hidden_dims': [50], 'learning_rate': 0.001, 'output_dim': 2, 'print': False}\n",
      "\n",
      "🧪 Evaluation on Test Set\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      0.87      0.85       451\n",
      "           1       0.75      0.69      0.72       255\n",
      "\n",
      "    accuracy                           0.80       706\n",
      "   macro avg       0.79      0.78      0.78       706\n",
      "weighted avg       0.80      0.80      0.80       706\n",
      "\n",
      "Accuracy : 0.80\n",
      "F1 Score : 0.72\n",
      "Recall   : 0.69\n",
      "Precision: 0.75\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/sklearn/pipeline.py:62: FutureWarning: This Pipeline instance is not fitted yet. Call 'fit' with appropriate arguments before using other methods such as transform, predict, etc. This will raise an error in 1.8 instead of the current warning.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "model = training_pipeline(4, train_fn, test = True, train = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 891,
     "referenced_widgets": [
      "b620ef17031a4519a190cfd286abe437",
      "d15f6a53f76a49f0b2ab925b25d1ba7c",
      "9e0f847caa054989b67645bfaacb6b80",
      "039339ffdd46487fb19c7b3f4ec0ba6c",
      "cd8b2c3cc17b40e9a7989470a85dcfee",
      "38b0b29f2aec4352a0a047873eed4019",
      "cbe2fb6f7eaa4cfab7acbc37b012c168",
      "8c634937a9994d50af40dbe0b5655557",
      "d2880c94c6764edea08b9da1637b4dc0",
      "59ad71ddb13a41978b0445447ff3967c",
      "63b929b344824b129b095edd5f006c87",
      "52b7470589344949a8cb3366a3943db0",
      "482d7ef320a1440894ec1127df7fe917",
      "95da4ced183543a0bf7627a4e7738bb6",
      "78b317ab75e8400f9d4533059001f532",
      "101fdfae2a5c4b6f92e3c99bdc248c30",
      "3b051205acc041c08e4336e32f2dbbb3",
      "50685a33c740476c89f18432a0390943",
      "3d4c027c5bd54b27ac0617a74d6489e8",
      "f5bd1223192648c4b07528a0269ac794",
      "735955c58d3540b78524d06996495135",
      "86919bbfdd1341c492b300aa1432df09"
     ]
    },
    "id": "mdcuIwFwUpas",
    "outputId": "ba59384a-8621-4a78-9cb8-8dbf9cb67d6a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔄 Preprocessing...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at prajjwal1/bert-mini and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b620ef17031a4519a190cfd286abe437",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2821 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-7-4e66316920a8>:183: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  self.trainer = Trainer(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚡ Fine-tuning BERT-mini (last-2 layers) …\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/transformers/utils/generic.py:271: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.\n",
      "  arr = np.array(obj)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1272' max='1272' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1272/1272 00:23, Epoch 4/4]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>318</td>\n",
       "      <td>0.532900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>636</td>\n",
       "      <td>0.330700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>954</td>\n",
       "      <td>0.300400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1272</td>\n",
       "      <td>0.288400</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/transformers/utils/generic.py:271: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.\n",
      "  arr = np.array(obj)\n",
      "/usr/local/lib/python3.11/dist-packages/transformers/utils/generic.py:271: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.\n",
      "  arr = np.array(obj)\n",
      "/usr/local/lib/python3.11/dist-packages/transformers/utils/generic.py:271: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.\n",
      "  arr = np.array(obj)\n",
      "Parameter 'function'=<function BertHFModel.predict.<locals>.<lambda> at 0x7e168a9162a0> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\n",
      "WARNING:datasets.fingerprint:Parameter 'function'=<function BertHFModel.predict.<locals>.<lambda> at 0x7e168a9162a0> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "52b7470589344949a8cb3366a3943db0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/706 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/transformers/utils/generic.py:271: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.\n",
      "  arr = np.array(obj)\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🧪 Evaluation on Test Set\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.98      0.91       451\n",
      "           1       0.94      0.71      0.81       255\n",
      "\n",
      "    accuracy                           0.88       706\n",
      "   macro avg       0.90      0.84      0.86       706\n",
      "weighted avg       0.89      0.88      0.88       706\n",
      "\n",
      "Accuracy : 0.88\n",
      "F1 Score : 0.81\n",
      "Recall   : 0.71\n",
      "Precision: 0.94\n"
     ]
    }
   ],
   "source": [
    "model = training_pipeline(5, train_fn, test = True, train = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DxYW5UpQMO4g",
    "outputId": "c7fc2a57-3cbd-4a2b-9c91-f744711670ee"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔄 Preprocessing...\n",
      "📦 Loaded vector types from config: ['tfidf', 'additional']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [06:42:19] WARNING: /workspace/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    }
   ],
   "source": [
    "model = retrain_best_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "neeMIQbUMcuN",
    "outputId": "e5a4a57e-27db-4908-b32e-744319b767e1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔄 Pre-Process Data\n",
      "📊 Predicting...\n",
      "1 1 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 1 1 0 0 1 0 1 1 0 0 0 1 0 0 0 1 1 1 0 0 0 0 1 1 1 0 0 0 1 1 0 0 0 1 0 0 0 0 0 0 0 1 0 1 1 1 0 0 0 0 0 1 1 0 1 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 1 0 1 1 0 0 0 1 0 0 1 0 0 1 0 0 1 0 0 1 0 0 0 0 0 0 1 0 0 1 0 1 1 0 0 0 1 0 1 1 1 1 0 0 0 1 0 1 0 0 1 1 0 0 0 1 0 1 0 0 0 1 0 0 0 0 0 1 0 0 1 0 1 1 0 0 1 0 1 1 1 0 0 0 0 0 1 0 0 0 1 0 1 0 0 1 0 0 0 0 0 0 1 0 0 0 0 0 0 1 0\n"
     ]
    }
   ],
   "source": [
    "# Get predictions\n",
    "res = predict(model, test_fn)\n",
    "\n",
    "# Convert predictions to space-separated string\n",
    "pred_string = ' '.join([str(pred) for pred in res])\n",
    "print(pred_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "M2HfMh-sczge",
    "outputId": "4f67d628-9fd8-4316-c357-de1950d05f62"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Predictions exported to predictions.txt\n"
     ]
    }
   ],
   "source": [
    "with open(\"predictions.txt\", \"w\") as f:\n",
    "    f.write(pred_string)\n",
    "\n",
    "print(\"✅ Predictions exported to predictions.txt\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
